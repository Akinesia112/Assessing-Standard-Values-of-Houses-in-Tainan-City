{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.notebook import tqdm,trange\n",
    "import torch.nn as nn\n",
    "from random import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('100計數_地段率1209.csv',encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.zeros((141,132,50))\n",
    "label=np.zeros((141,132,1))\n",
    "for x in range(len(data)):\n",
    "    for y in range(len(data[0])):\n",
    "        id=141*y+x\n",
    "        datadict={\n",
    "            0:'距最近火車站距離',\n",
    "            1:'活動中心',\n",
    "            2:'郵局',\n",
    "            3:'公車站',\n",
    "            4:'加油站',\n",
    "            5:'停車場',\n",
    "            6:'腳踏車',\n",
    "            7:'寺廟',\n",
    "            8:'大賣場',\n",
    "            9:'服飾',\n",
    "            10:'便利商店',\n",
    "            11:'超市美妝',\n",
    "            12:'電子賣場',\n",
    "            13:'銀行',\n",
    "            14:'ATM',\n",
    "            15:'公園',\n",
    "            16:'高中',\n",
    "            17:'國小',\n",
    "            18:'國中',\n",
    "            19:'補習班',\n",
    "            20:'食物',\n",
    "            21:'診所',\n",
    "            22:'大學',\n",
    "            23:'觀光景點',\n",
    "            24:'禮品百貨',\n",
    "            25:'醫院',\n",
    "            26:'嫌惡設施_危險',\n",
    "            27:'嫌惡設施_殯葬',\n",
    "            28:'嫌惡設施_髒亂',\n",
    "            29:'平均公告現值',\n",
    "            30:'加權容積率',\n",
    "            31:'加權建蔽率',\n",
    "            32:'國土利用_農業',\n",
    "            33:'國土利用_森林',\n",
    "            34:'國土利用_交通',\n",
    "            35:'國土利用_水利',\n",
    "            36:'國土利用_商業',\n",
    "            37:'國土利用_純住宅',\n",
    "            38:'國土利用_混合住宅',\n",
    "            39:'國土利用_製造業',\n",
    "            40:'國土利用_倉儲',\n",
    "            41:'國土利用_宗教',\n",
    "            42:'國土利用_殯葬',\n",
    "            43:'國土利用_其他建築用地',\n",
    "            44:'國土利用_學校',\n",
    "            45:'國土利用_醫療',\n",
    "            46:'國土利用_其他公共利用土地',\n",
    "            47:'國土利用_遊憩',\n",
    "            48:'國土利用_礦業',\n",
    "            49:'國土利用_其他',\n",
    "\n",
    "            \n",
    "        }\n",
    "        label[x][y][0]=df.iloc[id]['內插地段率']\n",
    "        for i in range(50):\n",
    "            data[x][y][i]=df.iloc[id][datadict[i]]\n",
    "            if  data[x][y][i]>0:\n",
    "                temp=1\n",
    "            else:\n",
    "                data[x][y][i]=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9484\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "308f33b20f894a69909f28a97a5e4a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "validnum=len(df[df['內插地段率']>0.1])\n",
    "print(validnum)\n",
    "validdata1=np.zeros((len(df[df['內插地段率']>0.1]),21,11,11))\n",
    "validdata2=np.zeros((len(df[df['內插地段率']>0.1]),7,21,21))\n",
    "validdata3=np.zeros((len(df[df['內插地段率']>0.1]),22))\n",
    "\n",
    "validlabel=np.zeros((len(df[df['內插地段率']>0.1]),1))\n",
    "counter=0\n",
    "for y in trange(len(data[0])):\n",
    "    for x in range(len(data)):\n",
    "        if label[x][y][0]>0.5:\n",
    "            for i in range(1,22):\n",
    "                tempx=x-5\n",
    "                tempy=y-5\n",
    "                for a in range(11):\n",
    "                    for b in range(11):\n",
    "                        try:\n",
    "                            validdata1[counter][i][a][b]+=data[tempx+a][tempy+b][i]\n",
    "                        except:\n",
    "                            continue\n",
    "            for i in range(22,29):\n",
    "                tempx=x-10\n",
    "                tempy=y-10\n",
    "                for a in range(21):\n",
    "                    for b in range(21):\n",
    "                        try:\n",
    "                            validdata1[counter][i][a][b]+=data[tempx+a][tempy+b][i]\n",
    "                        except:\n",
    "                            continue\n",
    "            \n",
    "            validlabel[counter][0]=label[x][y][0]-100\n",
    "            validdata3[counter][0]=data[x][y][0]\n",
    "            for i in range(29,50):\n",
    "                validdata3[counter][i-28]=data[x][y][i]\n",
    "            counter+=1\n",
    "\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class ExampleDataset(Dataset):\n",
    "\n",
    "    # data loading\n",
    "    def __init__(self,tr1,tr2,tr3,target, length):\n",
    "        \n",
    "        self.x1 = tr1.astype(np.float32)\n",
    "        self.x2 = tr2.astype(np.float32)\n",
    "        self.x3 = tr3.astype(np.float32)\n",
    "        self.y = target.astype(np.float32)\n",
    "        self.n_samples = length\n",
    "\n",
    "    # working for indexing\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return self.x1[index],self.x2[index],self.x3[index], self.y[index]\n",
    "\n",
    "    # return the length of our dataset\n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yehsanity\\AppData\\Local\\Temp\\ipykernel_15076\\1992625236.py:7: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  r=np.random.permutation(k)\n"
     ]
    }
   ],
   "source": [
    "k=[]\n",
    "\n",
    "for i in range(len(validdata3)):\n",
    "    k.append([validdata1[i],validdata2[i],validdata3[i],validlabel[i]])\n",
    "\n",
    "\n",
    "r=np.random.permutation(k)\n",
    "for i in range(len(validdata3)):\n",
    "    validdata1[i]=k[i][0]\n",
    "    validdata2[i]=k[i][1]\n",
    "    validdata3[i]=k[i][2]\n",
    "    validlabel[i]=k[i][3]\n",
    "tr0_set=ExampleDataset(validdata1[0:8500],validdata2[0:8500],validdata3[0:8500],validlabel[0:8500],8500)\n",
    "tr0_val=ExampleDataset(validdata1[8500:],validdata2[8500:],validdata3[8500:],validlabel[8500:],len(validdata3)-8500)\n",
    "#\n",
    "dataloader = DataLoader(tr0_set, batch_size=40, shuffle=True)\n",
    "dataloader_val = DataLoader(tr0_val, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class NeuralNet(nn.Module):\n",
    "    ''' A simple fully-connected deep neural network '''\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.cv2_1=nn.Conv2d(21, 64, 3, stride=1)\n",
    "        self.cv2_2=nn.Conv2d(64, 256, 5, stride=1)\n",
    "        self.cv2_3=nn.Conv2d(7, 32, 5, stride=1)\n",
    "        self.cv2_4=nn.Conv2d(32, 64, 10, stride=1)\n",
    "\n",
    "        self.linear1 = nn.Linear(22, 32)\n",
    "        self.linear2 = nn.Linear(32, 64)\n",
    "\n",
    "        self.layer1 = nn.Linear(5*5*256+8*8*64+64, 128)\n",
    "        self.layer2 = nn.Linear(128, 32)\n",
    "        self.out = nn.Linear(32, 1) \n",
    "\n",
    "        self.ba1=nn.BatchNorm3d(21)\n",
    "        self.ba2=nn.BatchNorm3d(7)\n",
    "        self.ba3=nn.BatchNorm1d(2)\n",
    "\n",
    "        self.act_fn1 = nn.ReLU()\n",
    "        self.act_fn2 = nn.ReLU()\n",
    "        self.act_fn3 = nn.ReLU()\n",
    "        self.act_fn4 = nn.ReLU()\n",
    "        self.act_fn5 = nn.ReLU()\n",
    "        # Define your neural network here\n",
    "        # TODO: How to modify this model to achieve better performance?\n",
    "        # self.net = nn.Sequential(\n",
    "        #     nn.Conv2d(28, 64, 3, stride=1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(64, 256, 5, stride=1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Flatten(),\n",
    "        #     nn.Linear(5*5*256, 128),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(128, 32),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(32, 1) \n",
    "        # )\n",
    "\n",
    "        # Mean squared error loss\n",
    "        self.criterion = nn.MSELoss(reduction='mean')\n",
    "\n",
    "    def forward(self, x1,x2,x3):\n",
    "        ''' Given input of size (batch_size x input_dim), compute output of the network '''\n",
    "        x1=self.cv2_1(x1)\n",
    "        x1=self.act_fn1(x1)\n",
    "        x1=self.cv2_2(x1)\n",
    "        x1=nn.Flatten()(x1)\n",
    "        x2=self.cv2_3(x2)\n",
    "        x2=self.act_fn2(x2)\n",
    "        x2=self.cv2_4(x2)\n",
    "        x2=nn.Flatten()(x2)\n",
    "        x3=self.linear1(x3)\n",
    "        x3=self.act_fn3(x3)\n",
    "        x3=self.linear2(x3)\n",
    "        x = torch.cat((x1,x2,x3), dim=1)\n",
    "        x = self.layer1(x)\n",
    "        x = self.act_fn4(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.act_fn5(x)\n",
    "        out=self.out(x)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def cal_loss(self, pred, target):\n",
    "        ''' Calculate loss '''\n",
    "        # TODO: you may implement L1/L2 regularization here\n",
    "        lamb = 0.0003\n",
    "        if lamb :\n",
    "          l2_reg = torch.tensor(0.).to('cuda')\n",
    "          for param in self.parameters() :\n",
    "            l2_reg += torch.norm(param)\n",
    "          return self.criterion(pred, target) + l2_reg*lamb\n",
    "        else : \n",
    "          return self.criterion(pred, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train(tr_set, val_set, model, device):\n",
    "    ''' DNN training '''\n",
    "    criterion = nn.MSELoss()\n",
    "    n_epochs = 70  # Maximum number of epochs\n",
    "    # Setup optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1E-5,weight_decay=0.00002)   \n",
    "\n",
    "    min_mse = 1000.\n",
    "    loss_record = {'train': [], 'dev': []}      # for recording training loss\n",
    "    early_stop_cnt = 0\n",
    "    epoch = 0\n",
    "    best_loss = 100000.0\n",
    "    while epoch < n_epochs:\n",
    "        train_loss = 0.0\n",
    "        val_acc = 0.0\n",
    "        val_loss = 0.0\n",
    "        model.train()                           # set model to training mode\n",
    "        for x1,x2,x3, y in tr_set:                     # iterate through the dataloader\n",
    "            optimizer.zero_grad()               # set gradient to zero\n",
    "            x1,x2,x3, labels = x1.to(device),x2.to(device),x3.to(device), y.to(device)   # move data to device (cpu/cuda)\n",
    "            outputs = model(x1,x2,x3)                     # forward pass (compute output)\n",
    "            mse_loss = criterion(outputs, labels)  # compute loss\n",
    "            _, train_pred = torch.max(outputs, 1)\n",
    "            mse_loss.backward()                 # compute gradient (backpropagation)\n",
    "            optimizer.step()                    # update model with optimizer\n",
    "            train_loss += mse_loss.item()\n",
    "\n",
    "        # After each epoch, test your model on the validation (development) set.\n",
    "        if len(val_set) > 0:\n",
    "            model.eval() # set the model to evaluation mode\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(val_set):\n",
    "                    x1,x2,x3, labels = data\n",
    "                    # print(data)\n",
    "                    x1,x2,x3, labels = x1.to(device),x2.to(device),x3.to(device), labels.to(device)\n",
    "                    outputs = model(x1,x2,x3)\n",
    "                    batch_loss = criterion(outputs, labels) \n",
    "                    _, val_pred = torch.max(outputs, 1) \n",
    "                    an= torch.max(labels) \n",
    "                    val_loss += batch_loss.item()\n",
    "\n",
    "                print('[{:03d}/{:03d}]  Loss: {:3.6f} | loss: {:3.6f}'.format(\n",
    "                    epoch + 1, n_epochs, train_loss/len(tr_set),  val_loss/len(val_set)\n",
    "                ))\n",
    "\n",
    "                # if the model improves, save a checkpoint at this epoch\n",
    "                if val_loss < best_loss:\n",
    "                    best_loss = val_loss\n",
    "                    torch.save(model, \"model_cnn_muti1210.pth\")\n",
    "                    print('saving model with loss {:.3f}'.format(best_loss/len(val_set)))\n",
    "        else:\n",
    "            print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f}'.format(\n",
    "                epoch + 1, n_epochs, train_loss/len(tr_set), train_loss/len(tr_set)\n",
    "            ))\n",
    "        epoch += 1\n",
    "    # if not validating, save the last epoch\n",
    "    if len(val_set) == 0:\n",
    "        torch.save(model, 'model.pth')\n",
    "        print('saving model at last epoch')\n",
    "\n",
    "    return min_mse, loss_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/070]  Loss: 177.897999 | loss: 119.306724\n",
      "[002/070]  Loss: 79.884084 | loss: 58.529214\n",
      "saving model with loss 58.529\n",
      "[003/070]  Loss: 68.196808 | loss: 51.832908\n",
      "saving model with loss 51.833\n",
      "[004/070]  Loss: 63.645501 | loss: 48.159798\n",
      "saving model with loss 48.160\n",
      "[005/070]  Loss: 60.499710 | loss: 49.342694\n",
      "[006/070]  Loss: 57.535521 | loss: 47.371032\n",
      "saving model with loss 47.371\n",
      "[007/070]  Loss: 55.162647 | loss: 46.195956\n",
      "saving model with loss 46.196\n",
      "[008/070]  Loss: 53.055661 | loss: 44.886973\n",
      "saving model with loss 44.887\n",
      "[009/070]  Loss: 51.543760 | loss: 43.701222\n",
      "saving model with loss 43.701\n",
      "[010/070]  Loss: 50.101362 | loss: 43.802491\n",
      "[011/070]  Loss: 49.136758 | loss: 43.193187\n",
      "saving model with loss 43.193\n",
      "[012/070]  Loss: 47.666691 | loss: 42.178416\n",
      "saving model with loss 42.178\n",
      "[013/070]  Loss: 46.912673 | loss: 42.985817\n",
      "[014/070]  Loss: 46.646532 | loss: 42.694877\n",
      "[015/070]  Loss: 45.405834 | loss: 43.097334\n",
      "[016/070]  Loss: 44.922274 | loss: 43.471854\n",
      "[017/070]  Loss: 44.484600 | loss: 43.077610\n",
      "[018/070]  Loss: 44.224133 | loss: 43.234027\n",
      "[019/070]  Loss: 43.794365 | loss: 43.166386\n",
      "[020/070]  Loss: 43.164454 | loss: 43.193579\n",
      "[021/070]  Loss: 42.683131 | loss: 43.761790\n",
      "[022/070]  Loss: 42.280315 | loss: 43.823049\n",
      "[023/070]  Loss: 42.152705 | loss: 45.303729\n",
      "[024/070]  Loss: 41.568117 | loss: 45.177253\n",
      "[025/070]  Loss: 41.301733 | loss: 43.693974\n",
      "[026/070]  Loss: 40.769072 | loss: 43.096646\n",
      "[027/070]  Loss: 41.251990 | loss: 43.316870\n",
      "[028/070]  Loss: 40.279352 | loss: 44.112600\n",
      "[029/070]  Loss: 39.818711 | loss: 43.490578\n",
      "[030/070]  Loss: 39.863716 | loss: 43.221709\n",
      "[031/070]  Loss: 39.716812 | loss: 44.703904\n",
      "[032/070]  Loss: 39.008778 | loss: 46.799897\n",
      "[033/070]  Loss: 39.255535 | loss: 46.102926\n",
      "[034/070]  Loss: 38.523859 | loss: 44.319353\n",
      "[035/070]  Loss: 38.097371 | loss: 45.175965\n",
      "[036/070]  Loss: 38.639525 | loss: 45.825756\n",
      "[037/070]  Loss: 37.807862 | loss: 45.279636\n",
      "[038/070]  Loss: 37.920376 | loss: 45.885962\n",
      "[039/070]  Loss: 37.223536 | loss: 46.316164\n",
      "[040/070]  Loss: 37.339634 | loss: 45.788663\n",
      "[041/070]  Loss: 36.600817 | loss: 45.518540\n",
      "[042/070]  Loss: 36.387363 | loss: 46.519921\n",
      "[043/070]  Loss: 36.339147 | loss: 46.523370\n",
      "[044/070]  Loss: 36.069492 | loss: 47.071133\n",
      "[045/070]  Loss: 35.761077 | loss: 46.913228\n",
      "[046/070]  Loss: 36.000515 | loss: 47.117743\n",
      "[047/070]  Loss: 35.425813 | loss: 48.348588\n",
      "[048/070]  Loss: 35.493127 | loss: 49.934592\n",
      "[049/070]  Loss: 35.260704 | loss: 47.131791\n",
      "[050/070]  Loss: 35.276641 | loss: 52.939602\n",
      "[051/070]  Loss: 34.778460 | loss: 47.958066\n",
      "[052/070]  Loss: 34.754304 | loss: 48.374789\n",
      "[053/070]  Loss: 34.414410 | loss: 47.655476\n",
      "[054/070]  Loss: 34.218449 | loss: 48.423522\n",
      "[055/070]  Loss: 33.800731 | loss: 49.745068\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = NeuralNet().to('cuda')  # Construct model and move to device\n",
    "model_loss, model_loss_record = train(dataloader, dataloader_val, model, 'cuda')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'NeuralNet' on <module '__main__'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39mmodel_cnn_muti2-120_nor.pth\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtest\u001b[39m(tt_set, model, device):\n\u001b[0;32m      3\u001b[0m     model\u001b[39m.\u001b[39meval()                                \u001b[39m# set model to evalutation mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yehsanity\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:712\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    710\u001b[0m             opened_file\u001b[39m.\u001b[39mseek(orig_position)\n\u001b[0;32m    711\u001b[0m             \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mload(opened_file)\n\u001b[1;32m--> 712\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m    713\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n",
      "File \u001b[1;32mc:\\Users\\yehsanity\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:1049\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1047\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1048\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[1;32m-> 1049\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m   1051\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[0;32m   1053\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\yehsanity\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:1042\u001b[0m, in \u001b[0;36m_load.<locals>.UnpicklerWrapper.find_class\u001b[1;34m(self, mod_name, name)\u001b[0m\n\u001b[0;32m   1040\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m mod_name \u001b[39m=\u001b[39m load_module_mapping\u001b[39m.\u001b[39mget(mod_name, mod_name)\n\u001b[1;32m-> 1042\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfind_class(mod_name, name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can't get attribute 'NeuralNet' on <module '__main__'>"
     ]
    }
   ],
   "source": [
    "\n",
    "model=torch.load(\"model_cnn_muti1210.pth\")\n",
    "def test(tt_set, model, device):\n",
    "    model.eval()                                # set model to evalutation mode\n",
    "    preds = []\n",
    "    for x1,x2,x3, y in tt_set:                            # iterate through the dataloader\n",
    "        x1,x2,x3 = x1.to(device),x2.to(device),x3.to(device)                       # move data to device (cpu/cuda)\n",
    "        with torch.no_grad():                   # disable gradient calculation\n",
    "            pred = model(x1,x2,x3)                     # forward pass (compute output)\n",
    "            preds.append(pred.detach().cpu())   # collect prediction\n",
    "    preds = torch.cat(preds, dim=0).numpy()     # concatenate all predictions and convert to a numpy array\n",
    "    return preds\n",
    "\n",
    "count=0\n",
    "prd_err=[]\n",
    "prd_set=ExampleDataset(validdata1,validdata2,validdata3,validlabel,len(validlabel))\n",
    "tt_set = DataLoader(prd_set, batch_size=1, shuffle=False)\n",
    "preds = test(tt_set, model, 'cuda')  # predict COVID-19 cases with your model\n",
    "for i in range(len(preds)):\n",
    "    if preds[i] - validlabel[i] > 10 or preds[i] - validlabel[i] < -10:\n",
    "        print(\"實際值:\",validlabel[i],\"預測結果:\",preds[i],\"id:\",i)\n",
    "        count+=1\n",
    "        prd_err.append(i)\n",
    "\n",
    "\n",
    "print(\"預測錯誤數量:\",count)\n",
    "prd_err=pd.DataFrame(prd_err)\n",
    "prd_err.to_csv(\"prd_err_cnn.csv\",index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [27], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m realdata3\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mzeros((\u001b[39mlen\u001b[39m(df),\u001b[39m3\u001b[39m))\n\u001b[0;32m      8\u001b[0m counter\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m----> 9\u001b[0m \u001b[39mfor\u001b[39;00m y \u001b[39min\u001b[39;00m trange(\u001b[39mlen\u001b[39m(data[\u001b[39m0\u001b[39;49m])):\n\u001b[0;32m     10\u001b[0m     \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(data)):\n\u001b[0;32m     11\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m22\u001b[39m):\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "#real data(all)\n",
    "\n",
    "\n",
    "realdata1=np.zeros((len(df),21,11,11))\n",
    "realdata2=np.zeros((len(df),7,21,21))\n",
    "realdata3=np.zeros((len(df),22))\n",
    "\n",
    "counter=0\n",
    "for y in trange(len(data[0])):\n",
    "    for x in range(len(data)):\n",
    "        for i in range(1,22):\n",
    "            tempx=x-5\n",
    "            tempy=y-5\n",
    "            for a in range(11):\n",
    "                for b in range(11):\n",
    "                    try:\n",
    "                        realdata1[counter][i][a][b]+=data[tempx+a][tempy+b][i]\n",
    "                    except:\n",
    "                        continue\n",
    "        for i in range(24,29):\n",
    "            tempx=x-10\n",
    "            tempy=y-10\n",
    "            for a in range(21):\n",
    "                for b in range(21):\n",
    "                    try:\n",
    "                        realdata1[counter][i][a][b]+=data[tempx+a][tempy+b][i]\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "        \n",
    "        \n",
    "        realdata3[counter][0]=data[x][y][0]\n",
    "        realdata3[counter][1]=data[x][y][29]\n",
    "        for i in range(29,50):\n",
    "            realdata3[counter][i-28]=data[x][y][i]\n",
    "        \n",
    "        counter=counter+1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prd_set2=ExampleDataset(realdata1,realdata2,realdata3,realdata3,len(realdata3))\n",
    "tt_set = DataLoader(prd_set2, batch_size=1, shuffle=False)\n",
    "preds = test(tt_set, model, 'cuda')  # predict COVID-19 cases with your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "err=[]\n",
    "pred=[]\n",
    "for i in range(len(preds)):\n",
    "    ##err.append(abs(preds[i][0]-reallabel[i][0]))\n",
    "    pred.append(preds[i][0]+100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18612\n"
     ]
    }
   ],
   "source": [
    "print(len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['預測值']=pred\n",
    "df.to_csv('100計數_地段率1209.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 1.0, 'left': 13373240.3256, 'top': 2635136.2672999995, 'right': 13373340.325599998, 'bottom': 2635036.2673, '活動中心': 0.0, '郵局': 0.0, '公車站': 0.0, '火車站': 0.0, '加油站': 0.0, '停車場': 0.0, '腳踏車': 0.0, '寺廟': 0.0, '大賣場': 0.0, '服飾': 0.0, '便利商店': 0.0, '超市美妝': 0.0, '電子賣場': 0.0, '銀行': 0.0, 'ATM': 0.0, '大學': 0.0, '高中': 0.0, '國小': 0.0, '國中': 0.0, '補習班': 0.0, '嫌惡設施_危險': 0.0, '嫌惡設施_殯葬': 0.0, '嫌惡設施_髒亂': 0.0, '食物': 0.0, '診所': 0.0, '醫院': 0.0, '禮品百貨': 0.0, '觀光景點': 0.0, '房屋稅數量': 0.0, '地段率總和': 0.0, '折舊年數總和': 0.0, '平均地段率': None, '平均折舊年數': None, 'EPSG3857_X': 13373290.325599998, 'EPSG3857_Y': 2635086.2672999995, '距最近火車站距離': 8583.1767164421, '公園': 0.0, '土地單價': None, '預測值': 108.66443061828613}\n",
      "{'id': 1.0, 'left': 13373240.3256, 'top': 2635136.2672999995, 'right': 13373340.325599998, 'bottom': 2635036.2673, '活動中心': 0.0, '郵局': 0.0, '公車站': 0.0, '火車站': 0.0, '加油站': 0.0, '停車場': 0.0, '腳踏車': 0.0, '寺廟': 0.0, '大賣場': 0.0, '服飾': 0.0, '便利商店': 0.0, '超市美妝': 0.0, '電子賣場': 0.0, '銀行': 0.0, 'ATM': 0.0, '大學': 0.0, '高中': 0.0, '國小': 0.0, '國中': 0.0, '補習班': 0.0, '嫌惡設施_危險': 0.0, '嫌惡設施_殯葬': 0.0, '嫌惡設施_髒亂': 0.0, '食物': 0.0, '診所': 0.0, '醫院': 0.0, '禮品百貨': 0.0, '觀光景點': 0.0, '房屋稅數量': 0.0, '地段率總和': 0.0, '折舊年數總和': 0.0, '平均地段率': None, '平均折舊年數': None, 'EPSG3857_X': 13373290.325599998, 'EPSG3857_Y': 2635086.2672999995, '距最近火車站距離': 8583.1767164421, '公園': 0.0, '土地單價': None, '預測值': 108.66443061828613}\n"
     ]
    }
   ],
   "source": [
    "import decimal\n",
    "import json\n",
    "with open('1210\\100計數_地段率1209.geojson', newline='',encoding=\"utf-8\") as jsonfile:\n",
    "    data = json.load(jsonfile)\n",
    "\n",
    "for i in range(len(preds)):\n",
    "    data['features'][i]['properties']['預測值']=pred[i]\n",
    "\n",
    "print(data['features'][0]['properties'])\n",
    "with open('100計數_地段率1209.geojson_pre.geojson', 'w',encoding=\"utf-8\") as json_obj:\n",
    "    json.dump(data, json_obj, ensure_ascii = False, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open('100計數_地段率1209.geojson_pre.geojson', newline='',encoding=\"utf-8\") as jsonfile2:\n",
    "    data2 = json.load(jsonfile2)\n",
    "print(data2['features'][0]['properties'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c612b39049da8bb355db22d4f9d3096605140482de7c5e219419a9a6066d486"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
