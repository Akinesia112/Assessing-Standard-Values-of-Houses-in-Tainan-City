{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.notebook import tqdm,trange\n",
    "import torch.nn as nn\n",
    "from random import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('100計數_地段率1115.csv',encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "print(np.isnan(df.iloc[0]['平均公告現值']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.zeros((141,132,32))\n",
    "label=np.zeros((141,132,1))\n",
    "for x in range(len(data)):\n",
    "    for y in range(len(data[0])):\n",
    "        id=141*y+x\n",
    "        datadict={\n",
    "            0:'距最近火車站距離',\n",
    "            1:'活動中心',\n",
    "            2:'郵局',\n",
    "            3:'公車站',\n",
    "            4:'加油站',\n",
    "            5:'停車場',\n",
    "            6:'腳踏車',\n",
    "            7:'寺廟',\n",
    "            8:'大賣場',\n",
    "            9:'服飾',\n",
    "            10:'便利商店',\n",
    "            11:'超市美妝',\n",
    "            12:'電子賣場',\n",
    "            13:'銀行',\n",
    "            14:'ATM',\n",
    "            15:'公園',\n",
    "            16:'高中',\n",
    "            17:'國小',\n",
    "            18:'國中',\n",
    "            19:'補習班',\n",
    "            20:'食物',\n",
    "            21:'診所',\n",
    "            22:'禮品百貨',\n",
    "            23:'醫院',\n",
    "            24:'嫌惡設施_危險',\n",
    "            25:'嫌惡設施_殯葬',\n",
    "            26:'嫌惡設施_髒亂',\n",
    "            27:'大學',\n",
    "            28:'觀光景點',\n",
    "            29:'平均公告現值',\n",
    "            30:'加權容積率',\n",
    "            31:'加權建蔽率',\n",
    "            \n",
    "        }\n",
    "        label[x][y][0]=df.iloc[id]['平均地段率']\n",
    "        for i in range(32):\n",
    "            data[x][y][i]=df.iloc[id][datadict[i]]\n",
    "            if  data[x][y][i]>0:\n",
    "                temp=1\n",
    "            else:\n",
    "                data[x][y][i]=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4311\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0469e26e679d4fadbe5321cfaa1d0fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "validnum=len(df[df['平均地段率']>0.1])\n",
    "print(validnum)\n",
    "validdata1=np.zeros((len(df[df['平均地段率']>0.1]),21,11,11))\n",
    "validdata2=np.zeros((len(df[df['平均地段率']>0.1]),7,21,21))\n",
    "validdata3=np.zeros((len(df[df['平均地段率']>0.1]),4))\n",
    "\n",
    "validlabel=np.zeros((len(df[df['平均地段率']>0.1]),1))\n",
    "counter=0\n",
    "for y in trange(len(data[0])):\n",
    "    for x in range(len(data)):\n",
    "        if label[x][y][0]>0.5:\n",
    "            for i in range(1,22):\n",
    "                tempx=x-5\n",
    "                tempy=y-5\n",
    "                for a in range(11):\n",
    "                    for b in range(11):\n",
    "                        try:\n",
    "                            validdata1[counter][i][a][b]+=data[tempx+a][tempy+b][i]\n",
    "                        except:\n",
    "                            continue\n",
    "            for i in range(22,29):\n",
    "                tempx=x-10\n",
    "                tempy=y-10\n",
    "                for a in range(21):\n",
    "                    for b in range(21):\n",
    "                        try:\n",
    "                            validdata1[counter][i][a][b]+=data[tempx+a][tempy+b][i]\n",
    "                        except:\n",
    "                            continue\n",
    "            \n",
    "            validlabel[counter][0]=label[x][y][0]-120\n",
    "            validdata3[counter][0]=data[x][y][0]\n",
    "            validdata3[counter][1]=data[x][y][29]\n",
    "            validdata3[counter][2]=data[x][y][30]\n",
    "            validdata3[counter][3]=data[x][y][31]\n",
    "\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class ExampleDataset(Dataset):\n",
    "\n",
    "    # data loading\n",
    "    def __init__(self,tr1,tr2,tr3,target, length):\n",
    "        \n",
    "        self.x1 = tr1.astype(np.float32)\n",
    "        self.x2 = tr2.astype(np.float32)\n",
    "        self.x3 = tr3.astype(np.float32)\n",
    "        self.y = target.astype(np.float32)\n",
    "        self.n_samples = length\n",
    "\n",
    "    # working for indexing\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return self.x1[index],self.x2[index],self.x3[index], self.y[index]\n",
    "\n",
    "    # return the length of our dataset\n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yehsanity\\AppData\\Local\\Temp\\ipykernel_24236\\4285765929.py:7: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  r=np.random.permutation(k)\n"
     ]
    }
   ],
   "source": [
    "k=[]\n",
    "\n",
    "for i in range(len(validdata3)):\n",
    "    k.append([validdata1[i],validdata2[i],validdata3[i],validlabel[i]])\n",
    "\n",
    "\n",
    "r=np.random.permutation(k)\n",
    "for i in range(len(validdata3)):\n",
    "    validdata1[i]=k[i][0]\n",
    "    validdata2[i]=k[i][1]\n",
    "    validdata3[i]=k[i][2]\n",
    "    validlabel[i]=k[i][3]\n",
    "tr0_set=ExampleDataset(validdata1[0:2200],validdata2[0:2200],validdata3[0:2200],validlabel[0:2200],2200)\n",
    "tr0_val=ExampleDataset(validdata1[2200:],validdata2[2200:],validdata3[2200:],validlabel[2200:],len(validdata3)-2200)\n",
    "#\n",
    "dataloader = DataLoader(tr0_set, batch_size=30, shuffle=True)\n",
    "dataloader_val = DataLoader(tr0_val, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class NeuralNet(nn.Module):\n",
    "    ''' A simple fully-connected deep neural network '''\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.cv2_1=nn.Conv2d(21, 64, 3, stride=1)\n",
    "        self.cv2_2=nn.Conv2d(64, 256, 5, stride=1)\n",
    "        self.cv2_3=nn.Conv2d(7, 32, 5, stride=1)\n",
    "        self.cv2_4=nn.Conv2d(32, 64, 10, stride=1)\n",
    "\n",
    "        self.linear1 = nn.Linear(4, 8)\n",
    "        self.linear2 = nn.Linear(8, 8)\n",
    "\n",
    "        self.layer1 = nn.Linear(5*5*256+8*8*64+8, 128)\n",
    "        self.layer2 = nn.Linear(128, 32)\n",
    "        self.out = nn.Linear(32, 1) \n",
    "\n",
    "        self.ba1=nn.BatchNorm3d(21)\n",
    "        self.ba2=nn.BatchNorm3d(7)\n",
    "        self.ba3=nn.BatchNorm1d(2)\n",
    "\n",
    "        self.act_fn1 = nn.ReLU()\n",
    "        self.act_fn2 = nn.ReLU()\n",
    "        self.act_fn3 = nn.ReLU()\n",
    "        self.act_fn4 = nn.ReLU()\n",
    "        self.act_fn5 = nn.ReLU()\n",
    "        # Define your neural network here\n",
    "        # TODO: How to modify this model to achieve better performance?\n",
    "        # self.net = nn.Sequential(\n",
    "        #     nn.Conv2d(28, 64, 3, stride=1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(64, 256, 5, stride=1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Flatten(),\n",
    "        #     nn.Linear(5*5*256, 128),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(128, 32),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(32, 1) \n",
    "        # )\n",
    "\n",
    "        # Mean squared error loss\n",
    "        self.criterion = nn.MSELoss(reduction='mean')\n",
    "\n",
    "    def forward(self, x1,x2,x3):\n",
    "        ''' Given input of size (batch_size x input_dim), compute output of the network '''\n",
    "        x1=self.cv2_1(x1)\n",
    "        x1=self.act_fn1(x1)\n",
    "        x1=self.cv2_2(x1)\n",
    "        x1=nn.Flatten()(x1)\n",
    "        x2=self.cv2_3(x2)\n",
    "        x2=self.act_fn2(x2)\n",
    "        x2=self.cv2_4(x2)\n",
    "        x2=nn.Flatten()(x2)\n",
    "        x3=self.linear1(x3)\n",
    "        x3=self.act_fn3(x3)\n",
    "        x3=self.linear2(x3)\n",
    "        x = torch.cat((x1,x2,x3), dim=1)\n",
    "        x = self.layer1(x)\n",
    "        x = self.act_fn4(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.act_fn5(x)\n",
    "        out=self.out(x)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def cal_loss(self, pred, target):\n",
    "        ''' Calculate loss '''\n",
    "        # TODO: you may implement L1/L2 regularization here\n",
    "        lamb = 0.0003\n",
    "        if lamb :\n",
    "          l2_reg = torch.tensor(0.).to('cuda')\n",
    "          for param in self.parameters() :\n",
    "            l2_reg += torch.norm(param)\n",
    "          return self.criterion(pred, target) + l2_reg*lamb\n",
    "        else : \n",
    "          return self.criterion(pred, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train(tr_set, val_set, model, device):\n",
    "    ''' DNN training '''\n",
    "    criterion = nn.MSELoss()\n",
    "    n_epochs = 50  # Maximum number of epochs\n",
    "    best_loss = 0.0\n",
    "    # Setup optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1E-4,weight_decay=0.00002)   \n",
    "\n",
    "    min_mse = 1000.\n",
    "    loss_record = {'train': [], 'dev': []}      # for recording training loss\n",
    "    early_stop_cnt = 0\n",
    "    epoch = 0\n",
    "    best_loss = 100000.0\n",
    "    while epoch < n_epochs:\n",
    "        train_loss = 0.0\n",
    "        val_acc = 0.0\n",
    "        val_loss = 0.0\n",
    "        model.train()                           # set model to training mode\n",
    "        for x1,x2,x3, y in tr_set:                     # iterate through the dataloader\n",
    "            optimizer.zero_grad()               # set gradient to zero\n",
    "            x1,x2,x3, labels = x1.to(device),x2.to(device),x3.to(device), y.to(device)   # move data to device (cpu/cuda)\n",
    "            outputs = model(x1,x2,x3)                     # forward pass (compute output)\n",
    "            mse_loss = criterion(outputs, labels)  # compute loss\n",
    "            _, train_pred = torch.max(outputs, 1)\n",
    "            mse_loss.backward()                 # compute gradient (backpropagation)\n",
    "            optimizer.step()                    # update model with optimizer\n",
    "            train_loss += mse_loss.item()\n",
    "\n",
    "        # After each epoch, test your model on the validation (development) set.\n",
    "        if len(val_set) > 0:\n",
    "            model.eval() # set the model to evaluation mode\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(val_set):\n",
    "                    x1,x2,x3, labels = data\n",
    "                    x1,x2,x3, labels = x1.to(device),x2.to(device),x3.to(device), labels.to(device)\n",
    "                    outputs = model(x1,x2,x3)\n",
    "                    batch_loss = criterion(outputs, labels) \n",
    "                    _, val_pred = torch.max(outputs, 1) \n",
    "                    an= torch.max(labels) \n",
    "                    val_loss += batch_loss.item()\n",
    "\n",
    "                print('[{:03d}/{:03d}]  Loss: {:3.6f} | loss: {:3.6f}'.format(\n",
    "                    epoch + 1, n_epochs, train_loss/len(tr_set),  val_loss/len(val_set)\n",
    "                ))\n",
    "\n",
    "                # if the model improves, save a checkpoint at this epoch\n",
    "                if val_loss < best_loss:\n",
    "                    best_loss = val_loss\n",
    "                    torch.save(model, \"D:\\程式\\google map api\\model_cnn_muti2.pth\")\n",
    "                    print('saving model with loss {:.3f}'.format(best_loss/len(val_set)))\n",
    "        else:\n",
    "            print('[{:03d}/{:03d}] Train Acc: {:3.6f} Loss: {:3.6f}'.format(\n",
    "                epoch + 1, n_epochs, train_loss/len(tr_set), train_loss/len(tr_set)\n",
    "            ))\n",
    "        epoch += 1\n",
    "    # if not validating, save the last epoch\n",
    "    if len(val_set) == 0:\n",
    "        torch.save(model, 'model.pth')\n",
    "        print('saving model at last epoch')\n",
    "\n",
    "    return min_mse, loss_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/050]  Loss: 6.478383 | loss: 0.000129\n",
      "saving model with loss 0.000\n",
      "[002/050]  Loss: 2.206494 | loss: 0.000001\n",
      "saving model with loss 0.000\n",
      "[003/050]  Loss: 5.315324 | loss: 0.000005\n",
      "[004/050]  Loss: 0.577614 | loss: 0.000001\n",
      "saving model with loss 0.000\n",
      "[005/050]  Loss: 0.280075 | loss: 0.000043\n",
      "[006/050]  Loss: 0.023174 | loss: 0.000000\n",
      "saving model with loss 0.000\n",
      "[007/050]  Loss: 0.105275 | loss: 0.000007\n",
      "[008/050]  Loss: 0.164901 | loss: 0.000046\n",
      "[009/050]  Loss: 0.067839 | loss: 0.000000\n",
      "[010/050]  Loss: 0.000006 | loss: 0.000000\n",
      "saving model with loss 0.000\n",
      "[011/050]  Loss: 0.000002 | loss: 0.000000\n",
      "[012/050]  Loss: 0.000001 | loss: 0.000000\n",
      "[013/050]  Loss: 0.000000 | loss: 0.000000\n",
      "[014/050]  Loss: 0.000001 | loss: 0.000000\n",
      "[015/050]  Loss: 0.000010 | loss: 0.000000\n",
      "[016/050]  Loss: 0.000012 | loss: 0.000000\n",
      "[017/050]  Loss: 0.000017 | loss: 0.000000\n",
      "[018/050]  Loss: 0.000034 | loss: 0.000000\n",
      "saving model with loss 0.000\n",
      "[019/050]  Loss: 0.000081 | loss: 0.000000\n",
      "saving model with loss 0.000\n",
      "[020/050]  Loss: 0.000200 | loss: 0.000000\n",
      "[021/050]  Loss: 0.000652 | loss: 0.000000\n",
      "[022/050]  Loss: 0.054523 | loss: 0.000001\n",
      "[023/050]  Loss: 0.157060 | loss: 0.000000\n",
      "[024/050]  Loss: 0.019387 | loss: 0.000000\n",
      "[025/050]  Loss: 0.018325 | loss: 0.000002\n",
      "[026/050]  Loss: 0.014229 | loss: 0.000000\n",
      "[027/050]  Loss: 0.013905 | loss: 0.000000\n",
      "[028/050]  Loss: 0.007816 | loss: 0.000000\n",
      "[029/050]  Loss: 0.005595 | loss: 0.000000\n",
      "[030/050]  Loss: 0.003095 | loss: 0.000001\n",
      "[031/050]  Loss: 0.001555 | loss: 0.000000\n",
      "[032/050]  Loss: 0.002921 | loss: 0.000001\n",
      "[033/050]  Loss: 0.008344 | loss: 0.000000\n",
      "[034/050]  Loss: 0.006105 | loss: 0.000000\n",
      "[035/050]  Loss: 0.005932 | loss: 0.000000\n",
      "[036/050]  Loss: 0.004643 | loss: 0.000010\n",
      "[037/050]  Loss: 0.008525 | loss: 0.000011\n",
      "[038/050]  Loss: 0.008319 | loss: 0.000000\n",
      "[039/050]  Loss: 0.008631 | loss: 0.000000\n",
      "[040/050]  Loss: 0.005875 | loss: 0.000002\n",
      "[041/050]  Loss: 0.006411 | loss: 0.000001\n",
      "[042/050]  Loss: 0.002226 | loss: 0.000000\n",
      "[043/050]  Loss: 0.000755 | loss: 0.000000\n",
      "[044/050]  Loss: 0.000602 | loss: 0.000000\n",
      "[045/050]  Loss: 0.000586 | loss: 0.000000\n",
      "[046/050]  Loss: 0.000720 | loss: 0.000000\n",
      "[047/050]  Loss: 0.001154 | loss: 0.000000\n",
      "[048/050]  Loss: 0.001698 | loss: 0.000000\n",
      "[049/050]  Loss: 0.003114 | loss: 0.000016\n",
      "[050/050]  Loss: 0.005172 | loss: 0.000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = NeuralNet().to('cuda')  # Construct model and move to device\n",
    "model_loss, model_loss_record = train(dataloader, dataloader_val, model, 'cuda')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "實際值: [10.] 預測結果: [157.3305] id: 0\n",
      "4311\n",
      "預測錯誤數量: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model=torch.load(\"model_muti1116.pth\")\n",
    "def test(tt_set, model, device):\n",
    "    model.eval()                                # set model to evalutation mode\n",
    "    preds = []\n",
    "    for x1,x2,x3, y in tt_set:                            # iterate through the dataloader\n",
    "        x1,x2,x3 = x1.to(device),x2.to(device),x3.to(device)                       # move data to device (cpu/cuda)\n",
    "        with torch.no_grad():                   # disable gradient calculation\n",
    "            pred = model(x1,x2,x3)                     # forward pass (compute output)\n",
    "            preds.append(pred.detach().cpu())   # collect prediction\n",
    "    preds = torch.cat(preds, dim=0).numpy()     # concatenate all predictions and convert to a numpy array\n",
    "    return preds\n",
    "\n",
    "count=0\n",
    "prd_err=[]\n",
    "prd_set=ExampleDataset(validdata1,validdata2,validdata3,validlabel,len(validlabel))\n",
    "tt_set = DataLoader(prd_set, batch_size=1, shuffle=False)\n",
    "preds = test(tt_set, model, 'cuda')  # predict COVID-19 cases with your model\n",
    "for i in range(len(preds)):\n",
    "    if preds[i] - validlabel[i] > 10 or preds[i] - validlabel[i] < -10:\n",
    "        print(\"實際值:\",validlabel[i],\"預測結果:\",preds[i],\"id:\",i)\n",
    "        count+=1\n",
    "        prd_err.append(i)\n",
    "print(len(preds))\n",
    "\n",
    "print(\"預測錯誤數量:\",count)\n",
    "prd_err=pd.DataFrame(prd_err)\n",
    "prd_err.to_csv(\"prd_err_cnn.csv\",index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baac029b0002414fa89f58dd317d000e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/132 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#real data(all)\n",
    "\n",
    "\n",
    "realdata1=np.zeros((len(df),21,11,11))\n",
    "realdata2=np.zeros((len(df),7,21,21))\n",
    "realdata3=np.zeros((len(df),4))\n",
    "\n",
    "counter=0\n",
    "for y in trange(len(data[0])):\n",
    "    for x in range(len(data)):\n",
    "        for i in range(1,22):\n",
    "            tempx=x-5\n",
    "            tempy=y-5\n",
    "            for a in range(11):\n",
    "                for b in range(11):\n",
    "                    try:\n",
    "                        realdata1[counter][i][a][b]+=data[tempx+a][tempy+b][i]\n",
    "                    except:\n",
    "                        continue\n",
    "        for i in range(22,29):\n",
    "            tempx=x-10\n",
    "            tempy=y-10\n",
    "            for a in range(21):\n",
    "                for b in range(21):\n",
    "                    try:\n",
    "                        realdata1[counter][i][a][b]+=data[tempx+a][tempy+b][i]\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "        \n",
    "        realdata3[counter][0]=data[x][y][0]\n",
    "        realdata3[counter][1]=data[x][y][29]\n",
    "        realdata3[counter][2]=data[x][y][30]\n",
    "        realdata3[counter][3]=data[x][y][31]\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prd_set2=ExampleDataset(realdata1,realdata2,realdata3,realdata3,len(realdata3))\n",
    "tt_set = DataLoader(prd_set2, batch_size=1, shuffle=False)\n",
    "preds = test(tt_set, model, 'cuda')  # predict COVID-19 cases with your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "err=[]\n",
    "pred=[]\n",
    "for i in range(len(preds)):\n",
    "    ##err.append(abs(preds[i][0]-reallabel[i][0]))\n",
    "    pred.append(preds[i][0]+120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18612\n"
     ]
    }
   ],
   "source": [
    "print(len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['預測值']=pred\n",
    "df.to_csv('result100all_cnn_muti_120.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 1.0, 'left': 13373240.3256, 'top': 2635136.2672999995, 'right': 13373340.325599998, 'bottom': 2635036.2673, '活動中心': 0.0, '郵局': 0.0, '公車站': 0.0, '火車站': 0.0, '加油站': 0.0, '停車場': 0.0, '腳踏車': 0.0, '寺廟': 0.0, '大賣場': 0.0, '服飾': 0.0, '便利商店': 0.0, '超市美妝': 0.0, '電子賣場': 0.0, '銀行': 0.0, 'ATM': 0.0, '大學': 0.0, '高中': 0.0, '國小': 0.0, '國中': 0.0, '補習班': 0.0, '嫌惡設施_危險': 0.0, '嫌惡設施_殯葬': 0.0, '嫌惡設施_髒亂': 0.0, '食物': 0.0, '診所': 0.0, '醫院': 0.0, '禮品百貨': 0.0, '觀光景點': 0.0, '房屋稅數量': 0.0, '地段率總和': 0.0, '折舊年數總和': 0.0, '平均地段率': None, '平均折舊年數': None, 'EPSG3857_X': 13373290.325599998, 'EPSG3857_Y': 2635086.2672999995, '距最近火車站距離': 8583.1767164421, '公園': 0.0, '土地單價': None, '預測值': 108.66443061828613}\n",
      "{'id': 1.0, 'left': 13373240.3256, 'top': 2635136.2672999995, 'right': 13373340.325599998, 'bottom': 2635036.2673, '活動中心': 0.0, '郵局': 0.0, '公車站': 0.0, '火車站': 0.0, '加油站': 0.0, '停車場': 0.0, '腳踏車': 0.0, '寺廟': 0.0, '大賣場': 0.0, '服飾': 0.0, '便利商店': 0.0, '超市美妝': 0.0, '電子賣場': 0.0, '銀行': 0.0, 'ATM': 0.0, '大學': 0.0, '高中': 0.0, '國小': 0.0, '國中': 0.0, '補習班': 0.0, '嫌惡設施_危險': 0.0, '嫌惡設施_殯葬': 0.0, '嫌惡設施_髒亂': 0.0, '食物': 0.0, '診所': 0.0, '醫院': 0.0, '禮品百貨': 0.0, '觀光景點': 0.0, '房屋稅數量': 0.0, '地段率總和': 0.0, '折舊年數總和': 0.0, '平均地段率': None, '平均折舊年數': None, 'EPSG3857_X': 13373290.325599998, 'EPSG3857_Y': 2635086.2672999995, '距最近火車站距離': 8583.1767164421, '公園': 0.0, '土地單價': None, '預測值': 108.66443061828613}\n"
     ]
    }
   ],
   "source": [
    "import decimal\n",
    "import json\n",
    "with open('100計數_地段率1115.geojson', newline='',encoding=\"utf-8\") as jsonfile:\n",
    "    data = json.load(jsonfile)\n",
    "\n",
    "for i in range(len(preds)):\n",
    "    data['features'][i]['properties']['預測值']=pred[i]\n",
    "\n",
    "print(data['features'][0]['properties'])\n",
    "with open('100計數_地段率1115_pred.geojson', 'w',encoding=\"utf-8\") as json_obj:\n",
    "    json.dump(data, json_obj, ensure_ascii = False, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open('100計數_地段率1115_pred.geojson', newline='',encoding=\"utf-8\") as jsonfile2:\n",
    "    data2 = json.load(jsonfile2)\n",
    "print(data2['features'][0]['properties'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c612b39049da8bb355db22d4f9d3096605140482de7c5e219419a9a6066d486"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
